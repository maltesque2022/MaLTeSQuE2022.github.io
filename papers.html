
<!DOCTYPE HTML>
<!--
        Theory by TEMPLATED
        templated.co @templatedco
        Released for free under the Creative Commons Attribution 3.0 license (templated.co/license)
-->
<html>
<head>
    <title>MaLTeSQuE 2021</title>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <link rel="stylesheet" href="assets/css/main.css" />

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="#"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());

        gtag('config', 'UA-109528948-1');
    </script>


</head>
<body class="subpage">

<!-- Header -->
<header id="header">
    <div class="inner">
        <a href="index.html" class="logo"><h2><b style="color:white">MaLTeSQuE</b></h2></a>
        <nav id="nav">
            <a href="index.html"><h3 style="color:white">HOME</h3></a>
            <a href="submission.html"><h3 style="color:white">CALL FOR PAPERS</h3></a>
            <a href="papers.html"><h3><b style="color:white">ACCEPTED PAPERS</b></h3></a>
            <a href="program.html"><h3 style="color:white">PROGRAM</h3></a>
            <a href="committe.html"><h3 style="color:white">COMMITTEE</h3></a>
            <a href="https://maltesque2021.github.io/"><h3 style="color:white">PREVIOUS EDITION</h3></a>
        </nav>
        </nav>
        <a href="#navPanel" class="navPanelToggle"><span class="fa fa-bars"></span></a>
    </div>
</header>

<!-- Main -->
<section id="main" class="wrapper">
    <div class="inner" style="margin-top: -2% !important;">
        <header class="align-center">
            <h2>Accepted papers</h2>


            <p style="text-align: center;"><h4><strong>	Using Machine Learning to Guide the Application of Software Refactorings: A Preliminary Exploration</strong></h4></p>
                <p style="text-align: center;">
                    <li>Nikolaos Nikolaidis (University of Macedonia), Dimitris Zisis (Accenture),  Apostolos Ampatzoglou (University of Macedonia), Nikolaos Mittas (International Hellenic University), Alexander Chatzigeorgiou (University of Macedonia)</li>
            </p>

            <p><strong>Abstract:</strong> Refactorings constitute the most direct and comprehensible approach for addressing software quality issues, stemming directly from identified code smells. Nevertheless, despite their popularity in both the research and industrial communities: (a) the effect of a refactoring is not guaranteed to be successful; and (b) the plethora of available refactoring opportunities does not allow their comprehensive application. Thus, there is a need of guidance, on when to apply a refactoring opportunity, and when the development team shall postpone it. The notion of interest, forms one of the major pillars of the Technical Debt metaphor expressing the additional maintenance effort that will be required because of the accumulated debt. To assess the benefits of refactorings and guide when a refactoring should take place, we first present the results of an empirical study assessing and quantifying the impact of various refactorings on Technical Debt Interest (building a real-world training set) and use machine learning approaches for guiding the application of future refactorings. To estimate interest, we rely on the FITTED framework, which for each object-oriented class assesses its distance from the best-quality peer; whereas the refactorings that are applied throughout the history of a software project are extracted with the RefactoringMiner tool. The dataset of this study involves 4,166 refactorings applied accross 26,058 revisions of 10 Apache projects. The results suggest that the majority of refactorings reduce Technical Debt interest; however, considering all refactoring applications, it cannot be claimed that the mean impact differs from zero, confirming the results of previous studies highlighting mixed effects from the application of refactorings. To alleviate this problem, we have built an ade-quately accurate (~70%) model for the prediction of whether or not a refactoring should take place, in order to reduce Technical Debt interest.</p>

            <hr>



           <p style="text-align: center;"><h4><strong>  Are Machine Programming Systems Using Right Source-code Measures to Select Code Repositories?</strong></h4></p>
                <p style="text-align: center;">
                    <li>Niranjan Hasabnis (Intel Corporation)</li>
            </p>

            <p><strong>Abstract:</strong> Machine programming (MP) is an emerging field at the intersection of
deterministic and probabilistic computing, and it aims to assist software and
hardware engineers, among other applications. Along with powerful compute
resources, MP systems often rely on vast amount of open-source code to learn
interesting properties about code and programming and solve problems in the
areas of debugging, code recommendation, auto-completion, etc. Unfortunately,
several of the existing MP systems either do not consider quality of code
repositories or use atypical quality measures than those typically used in
software engineering community to select them. As such, impact of quality of
code repositories on the performance of these systems needs to be studied.

In this preliminary paper, we evaluate impact of sets of repositories of
different quality on the performance of a candidate MP system. Towards that
objective, we develop a framework, named GitRank, to rank open-source
repositories on quality, maintainability, and popularity by leveraging existing
research on this topic. We then apply GitRank to evaluate correlation
between the quality measures used by the candidate MP system and the quality
measures used by our framework. Our preliminary results reveal some correlation
between the quality measures used in GitRank and ControlFlag's performance,
suggesting that some of the measures used in GitRank are applicable to
ControlFlag. But the results also raise questions around right quality measures for code
repositories used in MP systems. We believe that our findings also generate
interesting insights towards code quality measures that affect performance of MP
systems.</p>

            <hr>





           <p style="text-align: center;"><h4><strong>  DeepCrash: Deep Metric Learning for Crash Bucketing Based on Stack Trace</strong></h4></p>
                <p style="text-align: center;">
                    <li>Liu Chao (SAP Labs), Xu Yang (SAP Labs), Xie Qiaoluan (SAP Labs), Li Yong (SAP Labs), Choi Hyun-Deok (SAP Labs)</li>
                </p>

            <p><strong>Abstract:</strong> Some software projects collect vast crash reports from testing and end users, then organize them in groups to efficiently fix bugs. This task is crash report bucketing. In particular, a high precision and fast speed crash similarity measurement approach is the critical constraint for large-scale crash bucketing. In this paper, we propose a deep learning-based crash bucketing method which maps stack trace to feature vectors and groups these feature vectors into buckets. First, we develop a frame tokenization method for stack trace, called frame2vec, to extract frame representations based on frame segmentation. Second, we propose a deep metric model to map the sequential stack trace representations into feature vectors whose similarity can represent the similarity of crashes. Third, a clustering algorithm is used to rapidly group similar feature vectors into same buckets to get the final result. Additionally, we evaluate our approach with the other seven competing methods on both private and public data sets. The results reveal that our method can speed up clustering and maintain high competitive precision.</p>

            <hr>




           <p style="text-align: center;"><h4><strong>  On the Application of Machine Learning Models to Assess and Predict Software Reusability</strong></h4></p>
                <p style="text-align: center;">
<li>Matthew Yit Hang Yeow (Monash University), Chun Yong Chong (Monash University),Mei Kuan Lim (Monash University)</li>

            </p>

            <p><strong>Abstract:</strong> Software reuse has been proven to be an effective strategy for developers to significantly increase software quality, reduce costs and increase the effectiveness of software development. Research in software reuse typically aims to address two main hurdles: 1.) reduce the time and effort required to identify reusable candidates, and 2.) avoid selecting low quality software components which may potentially lead to higher cost of development (i.e., solving bugs, errors, refactoring, etc.). Inherently, human judgment falls short in the aspect of reliability and effectiveness. Hence, in this paper, we investigate the applicability of Machine Learning (ML) algorithms in assessing software reusability. We collected more than 32k open- source projects and employed GitHub fork as the ground truth to its reusability. We have developed ML classification pipelines which are based on both internal and external software metrics to perform software reusability prediction. Our best performing ML classification model achieved an accuracy of 86%, outperforming existing research in both prediction performance and data coverage. Subsequently, we leverage our results by identifying key software characteristics that make software highly reusable. Our results show that size-related metrics (i.e., number of setters, methods, attributes) are the most impactful in contributing towards the reusability of the software.
            </p>

            <hr>






           <p style="text-align: center;"><h4><strong>  Neural Language Models for Code Quality Identification</strong></h4></p>
                <p style="text-align: center;">
<li>Srinivasan Sengamedu (Amazon), Hangqi Zhao (Twitter)</li>

            </p>

            <p><strong>Abstract:</strong> Neural Language Models for code have lead to interesting applications such as code completion and bug fix generation. Another type of code related application is the identification of code quality issues such as repetitive code and unnatural code. Neural language models contain implicit knowledge about such aspects. We propose a framework to detect code quality issues using neural language models. To handle repository-specific conventions, we use local or repository-specific models. The models are successful in detecting real-world code quality issues with low false positive rate.
            </p>

            <hr>





            <hr>

        </header>
    </div>
</section>

<!--
<section id="main" class="wrapper">
    <div class="inner" style="margin-top: -2% !important;">
        <header class="align-center">
            <h2>Accepted papers</h2>

            <h3>Technical papers</h3>

            <p style="text-align: center;"><h4><strong>A Preliminary Study on the Adequacy of Static Analysis Warnings with Respect to Code Smell Prediction</strong></h4></p>
            <p style="text-align: center;"><strong>Savanna Lujan</strong> <a style="color:blue" href="mailto:savanna.lujan@tuni.fi">savanna.lujan@tuni.fi</a> <i>(Tampere University)</i>, <strong>Fabiano Pecorelli</strong> <a style="color:blue" href="mailto:fpecorelli@unisa.it">fpecorelli@unisa.it</a> <i>(SeSa Lab - University of Salerno),</i><br>
            <strong>Fabio Palomba</strong> <a style="color:blue" href="mailto:fpalomba@unisa.it">fpalomba@unisa.it</a> <i>(SeSa Lab - University of Salerno)</i>, <strong>Andrea De Lucia</strong> <a style="color:blue" href="mailto:adelucia@unisa.it">adelucia@unisa.it</a> <i>(SeSa Lab - University of Salerno),</i><br>
                <strong>Valentina Lenarduzzi</strong> <a style="color:blue" href="mailto:valentina.lenarduzzi@lut.fi">valentina.lenarduzzi@lut.fi</a> <i>(LUT University)</i>
            </p>

            <p><strong>Abstract:</strong> Code smells are poor implementation choices applied during software evolution
                that can affect source code maintainability. While several heuristic-based approaches have been proposed
                in the past, machine learning solutions have recently gained attention since they may potentially address
                some limitations of state-of-the-art approaches. Unfortunately, however, machine learning-based code smell
                detectors still suffer from low accuracy. In this paper, we aim at advancing the knowledge in the field
                by investigating the role of static analysis warnings as features of machine learning models for the
                detection of three code smell types. We first verify the potential contribution given by these features.
                Then, we build code smell prediction models exploiting the most relevant features coming from the first
                analysis. The main finding of the study reports that the warnings given by the considered tools lead the
                performance of code smell prediction models to drastically increase with respect to what reported by previous
                research in the field.</p>

            <hr>

            <p style="text-align: center;"><h4><strong>RARE: A Labeled Dataset for Cloud-Native Memory Anomalies</strong></h4></p>
            <p style="text-align: center;"><strong>Francesco Lomio</strong> <a style="color:blue" href="mailto:francesco.lomio@tuni.fi">francesco.lomio@tuni.fi</a> <i>(Tampere University)</i>, <strong>Diego Mart√≠nez Baselga</strong> <a style="color:blue" href="mailto:diego.martinezbaselga@tuni.fi">diego.martinezbaselga@tuni.fi</a> <i>(Tampere University),</i><br>
                <strong>Sergio Moreschini</strong> <a style="color:blue" href="mailto:sergio.moreschini@tuni.fi">sergio.moreschini@tuni.fi</a> <i>(Tampere University)</i>, <strong>Heikki Huttunen</strong> <a style="color:blue" href="mailto:heikki.huttunen@tuni.fi">heikki.huttunen@tuni.fi</a> <i>(Tampere University),</i><br>
                <strong>Davide Taibi</strong> <a style="color:blue" href="mailto:davide.taibi@tuni.fi">davide.taibi@tuni.fi</a> <i>(Tampere University)</i>
            </p>

            <p><strong>Abstract:</strong> Anomaly detection has been attracting interest from both the industry and the research community for many years, as the number of published papers and services adopted grew exponentially over the last decade.
                One of the reasons behind this is the wide adoption of cloud systems from the majority of players in multiple industries, such as online shopping, advertisement or remote computing.
                In this work we propose a Dataset foR cloud-nAtive memoRy anomaliEs: RARE. It includes labelled anomaly time-series data, comprising of over 900 unique metrics.
                This dataset has been generated using a microservice for injecting artificial byte stream in order to overload the nodes, provoking memory anomalies, which in some cases resulted in a crash. The system was built using a Kafka server deployed on a Kubernetes system. Moreover, in order to get access and download the metrics related to the server, we utilised Prometheus.
                In this paper we present a dataset that can be used coupled with machine learning algorithms for detecting anomalies in a cloud based system. The dataset will be available in the form of CSV file through an online repository. Moreover, we also included an example of application using a Random Forest algorithm for classifying the data as anomalous or not. The goal of the RARE dataset is to help in the development of more accurate and reliable machine learning methods for anomaly detection in cloud based systems.
            </p>

            <hr>

            <p style="text-align: center;"><h4><strong>TraceSim: A Method for Calculating Stack Trace Similarity</strong></h4></p>
            <p style="text-align: center;"><strong>Roman Vasiliev</strong> <a style="color:blue" href="mailto:roman.vasiliev@jetbrains.com">roman.vasiliev@jetbrains.com</a> <i>(JetBrains)</i>, <strong>Dmitrij Koznov</strong> <a style="color:blue" href="mailto:d.koznov@spbu.ru">d.koznov@spbu.ru</a> <i>(Saint-Petersburg State University),</i><br>
                <strong>George Chernishev</strong> <a style="color:blue" href="mailto:chernishev@gmail.com">chernishev@gmail.com</a> <i>(Saint-Petersburg University, Russia)</i>, <strong>Aleksandr Khvorov</strong> <a style="color:blue" href="mailto:aleksandr.khvorov@jetbrains.com">aleksandr.khvorov@jetbrains.com</a> <i>(JetBrains),</i><br>
                <strong>Dmitry Luciv</strong> <a style="color:blue" href="mailto:d.luciv@spbu.ru">d.luciv@spbu.ru</a> <i>(Saint-Petersburg State University)</i>, <strong>Nikita Povarov</strong> <a style="color:blue" href="mailto:nikita.povarov@jetbrains.com">nikita.povarov@jetbrains.com</a> <i>(JetBrains)</i>
            </p>

            <p><strong>Abstract:</strong> Many contemporary software products have subsystems for automatic crash reporting.
                However, it is well-known that the same bug can produce slightly different reports. To manage this problem,
                reports are usually grouped, often manually by developers. Manual triaging, however, becomes infeasible for
                products that have large userbases, which is the reason for many different approaches to automating this task.
                Moreover, it is important to improve quality of triaging  due to the big volume of reports that needs to be
                processed properly. Therefore,  even a relatively small improvement could play a significant role in overall
                accuracy of report bucketing.
                The majority of existing studies use some kind of a stack trace similarity metric, either based on information
                retrieval techniques or string matching methods. However, it should be stressed that the quality of triaging is
                still insufficient.
                <br>
                In this paper, we describe TraceSim a novel approach to address this problem which combines TF-IDF, Levenshtein distance,
                and machine learning to construct a similarity metric. Our metric has been implemented inside an industrial-grade report
                triaging system. The evaluation on a manually labeled dataset shows significantly better results compared to baseline approaches.</p>

            <hr>

            <p style="text-align: center;"><h4><strong>Speeding Up the Data Extraction of Machine Learning Approaches: A Distributed Framework</strong></h4></p>
            <p style="text-align: center;"><strong>Martin Steinhauer</strong> <a style="color:blue" href="mailto:m.steinhauer@studenti.unisa.it">m.steinhauer@studenti.unisa.it</a> <i>(University of Salerno)</i>,
                <strong>Fabio Palomba</strong> <a style="color:blue" href="mailto:fpalomba@unisa.it">fpalomba@unisa.it</a> <i>(University of Salerno)</i><br></p>

            <p><strong>Abstract:</strong> In the last decade, mining software repositories (MSR) has become one of the most
                important sources to feed machine learning models. Especially open-source projects on platforms like GitHub
                are providing a tremendous amount of data and make them easily accessible. Nevertheless, there is still is a
                lack of standardized pipelines to extract data in an automated and fast way. Even though several frameworks
                and tools exist which can fulfill specific tasks or parts of the data extraction process, none of them allow
                neither building an automated mining pipeline nor the possibility for full parallelization. As a consequence,
                researchers interested in using mining software repositories to feed machine learning models are often forced
                to re-implement commonly used tasks leading to additional development time and libraries may not be integrated optimally.
                <br>
                This preliminary study aims to demonstrate current limitations of existing tools and Git itself which are
                threatening the prospects of standardization and parallelization. We also introduce the multi-dimensionality
                aspects of a Git repository and how they affects the computation time. Finally, as a proof of concept, we define
                an exemplary pipeline for predicting refactoring operations, assessing its performance. Finally, we discuss the
                limitations of the pipeline and further optimizations to be done.</p>

            <hr>

            <p style="text-align: center;"><h4><strong>Singling the Odd Ones Out: A Novelty Detection Approach to Find Defects in Infrastructure-as-Code</strong></h4></p>
            <p style="text-align: center;">
                <strong>Stefano Dalla Palma</strong> <a style="color:blue" href="mailto:s.dallapalma@uvt.nl">s.dallapalma@uvt.nl</a> <i>(Jheronimus Academy of Data Science)</i>, <strong>Majid Mohammadi</strong> <a style="color:blue" href="mailto:m.mohammadi1@tue.nl">m.mohammadi1@tue.nl</a> <i>(Jheronimus Academy of Data Science)</i>, <strong>Dario Di Nucci</strong> <a style="color:blue" href="mailto:d.dinucci@uvt.nl">d.dinucci@uvt.nl</a> <i>(Jheronimus Academy of Data Science)</i>, <strong>Damian A. Tamburri</strong> <a style="color:blue" href="mailto:d.a.tamburri@tue.nl">d.a.tamburri@tue.nl</a> <i>(Jheronimus Academy of Data Science)</i>
            </p>

            <p><strong>Abstract:</strong> Although Infrastructure-as-Code (IaC) is increasingly adopted, little is known about how to best maintain and evolve it.
                Previous studies focused on defining Machine-Learning models to predict defect-prone blueprints with the final goal of helping DevOps engineers scheduling testing and maintenance activities. However, the dominant technique for IaC defect prediction is supervised binary classification, which uses defective and non-defective instances for training but Such methods require labeled data points to train the classifier. Furthermore, the high imbalance between defective and non-defective samples makes the training more difficult and leads to unreliable classifiers. In this work, we tackle the defect-prediction problem from a different perspective using novelty detection and evaluate the performance of three techniques, namely OneClassSVM, LocalOutlierFactor, and IsolationForest, and compare their performance with a baseline RandomForest binary classifier.
                Such models are trained using only non-defective samples. At the same time, defective data points are treated as novelty because the number of defective samples is too little compared to defective ones.
                We conduct an empirical study on an extremely imbalanced dataset consisting of 85 real-world Ansible projects containing only small amounts of defective instances. We found that novelty detection techniques can recognize defects with a high level of precision and recall, an AUC-PR up to 0.86, and an MCC up to 0.31. We deem our results can influence the current trends in defect detection and put forward a new research path toward dealing with this problem.
            </p>

            <hr>

            <p style="text-align: center;"><h4><strong>DeepIaC: Deep Learning-based Linguistic Anti-pattern Detection in IaC</strong></h4></p>
            <p style="text-align: center;">
                <strong>Nemania Borovits</strong> <a style="color:blue" href="mailto:n.borovits@tilburguniversity.edu">n.borovits@tilburguniversity.edu</a> <i>(Tilburg University/JADS)</i>, <strong>Indika Kumara</strong> <a style="color:blue" href="mailto:i.p.k.weerasingha.dewage@tue.nl">i.p.k.weerasingha.dewage@tue.nl</a> <i>(Eindhoven University of Technology/JADS)</i>, <strong>Parvathy Krishnan</strong> <a style="color:blue" href="mailto:parvathykrishnank@gmail.com">parvathykrishnank@gmail.com</a> <i>(Tilburg University/JADS)</i>, <strong>Stefano Dalla Palma</strong> <a style="color:blue" href="mailto:s.dalla.palma@uvt.nl">s.dalla.palma@uvt.nl</a> <i>(Tilburg University/JADS)</i>, <strong>Dario Di Nucci</strong> <a style="color:blue" href="mailto:d.dinucci@uvt.nl">d.dinucci@uvt.nl</a> <i>(Tilburg University/JADS)</i>, <strong>Fabio Palomba</strong> <a style="color:blue" href="mailto:fpalomba@unisa.it">fpalomba@unisa.it</a> <i>(University of Salerno)</i>, <strong>Damian Andrew Tamburri</strong> <a style="color:blue" href="mailto:d.a.tamburri@tue.nl">d.a.tamburri@tue.nl</a> <i>(Eindhoven University of Technology/JADS)</i>, <strong>Willem-Jan van den Heuvel</strong> <a style="color:blue" href="mailto:W.J.A.M.v.d.Heuvel@jads.nl">W.J.A.M.v.d.Heuvel@jads.nl</a> <i>(Tilburg University/JADS)</i>
            </p>

            <p><strong>Abstract:</strong> Linguistic anti-patterns are recurring poor practices concerning inconsistencies
                among the naming, documentation, and implementation of an entity. They impede readability, understandability,
                and maintainability of source code. In this paper, we attempt to detect linguistic anti-patterns in infrastructure as code
                (IaC) scripts used to provision and manage computing environments. In particular, we consider inconsistencies between the
                logic/body of IaC code units and their names. To this end, we propose a novel automated approach that employs word embeddings
                and deep learning techniques. We build and use the abstract syntax tree of IaC code units to create their code embedments.
                Our experiments with a dataset systematically extracted from open source repositories show that our approach yields an accuracy
                between 0.785 and 0.915 in detecting inconsistencies.
            </p>

            <h3>Presentation abstract</h3>

            <p style="text-align: center;"><h4><strong>An Effective Sequence Alignment Method for Duplicate Crash Report Detection</strong></h4></p>
            <p style="text-align: center;">
                <strong>Irving Muller Rodrigues</strong> <a style="color:blue" href="mailto:irving.rodrigues@gmail.com">irving.rodrigues@gmail.com</a> <i>(Polytechnique Montreal)</i>, <strong>Daniel Aloise</strong> <a style="color:blue" href="mailto:daniel.aloise@polymtl.ca">daniel.aloise@polymtl.ca</a> <i>(Polytechnique Montreal)</i>,<br>
                <strong>Eraldo Rezende Fernandes</strong> <a style="color:blue" href="mailto:eraldo@facom.ufms.br">eraldo@facom.ufms.br</a> <i>(Universidade Federal de Mato Grosso do Sul)</i>
            </p>

            <p><strong>Abstract:</strong> Software systems can automatically send crash reports to developers for investigation
                when a program failure occurs. A significant portion of these crash reports are duplicate, i.e., they were caused
                by the same software issue. In general, developers want to group duplicate crash reports into the same cluster,
                denoted bucket, to better analyze the software failure. However, to manually perform this task is time consuming,
                laborious and impractical in many software systems. In this paper, we present a novel method to automatically detect duplicate
                crash reports based on stack traces generated when the system crashes. Our technique is an extension of a previous method based
                on the Needleman-Wunsch algorithm. This previous method computes the similarity between two stack traces by means of edit operations considering fixed penalties. We propose a mechanism that incorporates the position and the frequency of functions
                in the stack trace in order to compute these penalties. We demonstrate that our technique outperforms state-of-the-art
                systems and strong baselines in different scenarios.
            </p>

        </header>
    </div>
</section>

&lt;!&ndash; Footer &ndash;&gt;
<footer id="footer">
    <div class="inner">
        <div class="flex">
            <div class="copyright">
                &copy; Untitled. Design: <a href="https://templated.co">TEMPLATED</a>. Images: <a href="https://unsplash.com">Unsplash</a>.
            </div>
            <ul class="icons">
                <li><a href="https://www.facebook.com/Maltesque-Workshop-168364760413829/" class="icon fa-facebook" target="blank"><span class="label">Facebook</span></a></li>
                <li><a href="https://twitter.com/MaLTeSQuE_2018" class="icon fa-twitter"
                       target="blank" ><span class="label">Twitter</span></a></li>
            </ul>
        </div>
    </div>
</footer>
-->

		<!-- Footer -->
			<footer id="footer">
				<div class="inner">
					<div class="flex">
						<div class="copyright">
							&copy; Untitled. Design: <a href="https://templated.co">TEMPLATED</a>. Images: <a href="https://unsplash.com">Unsplash</a>.
						</div>
						<ul class="icons">
                        <li><a href="https://www.facebook.com/Maltesque-Workshop-168364760413829/" class="icon fa-facebook" target="blank"><span class="label">Facebook</span></a></li>
                        <li><a href="https://twitter.com/MaLTeSQuE_2021" class="icon fa-twitter"
                            target="blank" ><span class="label">Twitter</span></a></li>
                    </ul>
					</div>
				</div>
			</footer>

		<!-- Scripts -->
			<script src="assets/js/jquery.min.js"></script>
			<script src="assets/js/skel.min.js"></script>
			<script src="assets/js/util.js"></script>
			<script src="assets/js/main.js"></script>

	</body>
</html>
